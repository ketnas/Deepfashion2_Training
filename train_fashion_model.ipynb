{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"train_fashion_model.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyO0HNA/aymhy+4c9DwZDGpK"},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"code","metadata":{"id":"ONTvWSg1pf0V"},"source":["!rm -r Deepfashion2_Training/"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Xuo7rgRH1Enp","executionInfo":{"elapsed":7053,"status":"ok","timestamp":1609859250358,"user":{"displayName":"kejkaew thanasuan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh9QTKDSZ9uv61V23eh27Gu_jztkcap_wnhDLCbcQ=s64","userId":"01212051864468347195"},"user_tz":-420},"outputId":"4516adb9-7628-4068-efe5-472b241ffe97"},"source":["!git clone https://github.com/ketnas/Deepfashion2_Training.git"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Cloning into 'Deepfashion2_Training'...\n","remote: Enumerating objects: 12, done.\u001b[K\n","remote: Counting objects: 100% (12/12), done.\u001b[K\n","remote: Compressing objects: 100% (9/9), done.\u001b[K\n","remote: Total 110 (delta 6), reused 7 (delta 3), pack-reused 98\u001b[K\n","Receiving objects: 100% (110/110), 74.80 MiB | 31.89 MiB/s, done.\n","Resolving deltas: 100% (11/11), done.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Oq518fn-eUdN","executionInfo":{"elapsed":6731,"status":"ok","timestamp":1609860905729,"user":{"displayName":"kejkaew thanasuan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh9QTKDSZ9uv61V23eh27Gu_jztkcap_wnhDLCbcQ=s64","userId":"01212051864468347195"},"user_tz":-420},"outputId":"b6bc4fc7-274b-40eb-dfb5-cd4ca58fccd6"},"source":["%tensorflow_version 1.x\n","import tensorflow as tf\n","print(tf.__version__)\n","\n","device_name = tf.test.gpu_device_name()\n","if device_name != '/device:GPU:0':\n","  raise SystemError('GPU device not found')\n","print('Found GPU at: {}'.format(device_name))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["TensorFlow 1.x selected.\n","1.15.2\n","Found GPU at: /device:GPU:0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ll72zQxZ1i3l"},"source":["from google.colab import auth\n","auth.authenticate_user()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ye2PJJhO476E"},"source":["!curl https://sdk.cloud.google.com | bash"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"n1yqe5I35Be5"},"source":["!gcloud init"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8ZgINyAu5GIf","executionInfo":{"elapsed":2932,"status":"ok","timestamp":1609857984786,"user":{"displayName":"kejkaew thanasuan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh9QTKDSZ9uv61V23eh27Gu_jztkcap_wnhDLCbcQ=s64","userId":"01212051864468347195"},"user_tz":-420},"outputId":"844a35ef-c677-44b5-bdd3-e82c914ee455"},"source":["!gsutil ls"],"execution_count":null,"outputs":[{"output_type":"stream","text":["gs://fashion_test/\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Yrq-0lJMCKky"},"source":["!mkdir Dataset"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SY-AGQWZ5KC9","executionInfo":{"elapsed":293193,"status":"ok","timestamp":1609858674144,"user":{"displayName":"kejkaew thanasuan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh9QTKDSZ9uv61V23eh27Gu_jztkcap_wnhDLCbcQ=s64","userId":"01212051864468347195"},"user_tz":-420},"outputId":"9ff58dd0-2224-44a2-8ed5-fd247a0e0129"},"source":["!gsutil cp gs://fashion_test/train.zip ./Dataset\n","!gsutil cp gs://fashion_test/validation.zip ./Dataset"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Copying gs://fashion_test/train.zip...\n","\\ [1 files][  9.9 GiB/  9.9 GiB]   39.3 MiB/s                                   \n","Operation completed over 1 objects/9.9 GiB.                                      \n","Copying gs://fashion_test/validation.zip...\n","- [1 files][  1.7 GiB/  1.7 GiB]   11.0 MiB/s                                   \n","Operation completed over 1 objects/1.7 GiB.                                      \n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"mw6wkyOb5TWI"},"source":["!unzip -P 2019Deepfashion2** ./Dataset/train.zip\n","!unzip -P 2019Deepfashion2** ./Dataset/validation.zip"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bvs_f1q8kCN9"},"source":["!mv ./train ./Dataset"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"T0M4RcMYkWjo"},"source":["!mv ./validation ./Dataset"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"19yKP1106DKX","executionInfo":{"elapsed":47629,"status":"ok","timestamp":1609859151525,"user":{"displayName":"kejkaew thanasuan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh9QTKDSZ9uv61V23eh27Gu_jztkcap_wnhDLCbcQ=s64","userId":"01212051864468347195"},"user_tz":-420},"outputId":"084d1b33-3791-4af8-f8fd-4ed75f20f4cf"},"source":["!gsutil cp gs://fashion_test/train.json ./Dataset/train\n","!gsutil cp gs://fashion_test/valid.json ./Dataset/validation"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Copying gs://fashion_test/train.json...\n","\\ [1 files][  1.5 GiB/  1.5 GiB]   44.0 MiB/s                                   \n","Operation completed over 1 objects/1.5 GiB.                                      \n","Copying gs://fashion_test/valid.json...\n","/ [1 files][386.7 MiB/386.7 MiB]                                                \n","Operation completed over 1 objects/386.7 MiB.                                    \n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"EVcoAGoyGd5e"},"source":["**Start from training from here**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9rvsvAaG6Kw5","executionInfo":{"elapsed":2618,"status":"ok","timestamp":1609860911965,"user":{"displayName":"kejkaew thanasuan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh9QTKDSZ9uv61V23eh27Gu_jztkcap_wnhDLCbcQ=s64","userId":"01212051864468347195"},"user_tz":-420},"outputId":"5ffb3bd1-6c0e-4b26-8533-8a282938bc2d"},"source":["import random\n","import math\n","import re\n","import time\n","import numpy as np\n","import cv2\n","import matplotlib\n","import matplotlib.pyplot as plt\n","\n","import sys\n","sys.dont_write_bytecode = True\n","\n","import os\n","import numpy as np\n","\n","from pycocotools.coco import COCO\n","from pycocotools import mask as maskUtils\n","from Deepfashion2_Training.lib.config import Config\n","from Deepfashion2_Training.lib.model import MaskRCNN\n","from Deepfashion2_Training.lib import utils\n","\n","%matplotlib inline\n","\n","ROOT_DIR = os.path.abspath(\"./\")\n","\n","# Directory to save logs and trained model\n","MODEL_DIR = os.path.join(ROOT_DIR, \"logs\")\n","\n","# Local path to trained weights file\n","COCO_MODEL_PATH = os.path.join(ROOT_DIR, \"mask_rcnn_coco.h5\")\n","# Download COCO trained weights from Releases if needed\n","if not os.path.exists(COCO_MODEL_PATH):\n","    utils.download_trained_weights(COCO_MODEL_PATH)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"2mbnrtgbl4gB"},"source":["class DeepFashion2Config(Config):\n","    \"\"\"Configuration for training on DeepFashion2.\n","    Derives from the base Config class and overrides values specific\n","    to the DeepFashion2 dataset.\n","    \"\"\"\n","    # Give the configuration a recognizable name\n","    NAME = \"deepfashion2\"\n","\n","    # We use a GPU with 12GB memory, which can fit two images.\n","    # Adjust down if you use a smaller GPU.\n","    IMAGES_PER_GPU = 1\n","\n","    # Uncomment to train on 8 GPUs (default is 1)\n","    #GPU_COUNT = 4\n","    GPU_COUNT = 1\n","\n","    # Number of classes (including background)\n","    NUM_CLASSES = 1 + 13  # COCO has 80 classes\n","    \n","    USE_MINI_MASK = True\n","\n","    train_img_dir = \"./Dataset/train/image\"\n","    train_json_path = \"./Dataset/train/train.json\"\n","    valid_img_dir = \"./Dataset/validation/image\"\n","    valid_json_path = \"./Dataset/validation/valid.json\"\n","\n","\n","############################################################\n","#  Dataset\n","############################################################\n","\n","class DeepFashion2Dataset(utils.Dataset):\n","    def load_coco(self, image_dir, json_path, class_ids=None,\n","                  class_map=None, return_coco=False):\n","        \"\"\"Load the DeepFashion2 dataset.\n","        \"\"\"\n","\n","        coco = COCO(json_path)\n","\n","        # Load all classes or a subset?\n","        if not class_ids:\n","            # All classes\n","            class_ids = sorted(coco.getCatIds())\n","\n","        # All images or a subset?\n","        if class_ids:\n","            image_ids = []\n","            for id in class_ids:\n","                image_ids.extend(list(coco.getImgIds(catIds=[id])))\n","            # Remove duplicates\n","            image_ids = list(set(image_ids))\n","        else:\n","            # All images\n","            image_ids = list(coco.imgs.keys())\n","\n","        # Add classes\n","        for i in class_ids:\n","            self.add_class(\"deepfashion2\", i, coco.loadCats(i)[0][\"name\"])\n","\n","        # Add images\n","        for i in image_ids:\n","            self.add_image(\n","                \"deepfashion2\", image_id=i,\n","                path=os.path.join(image_dir, coco.imgs[i]['file_name']),\n","                width=coco.imgs[i][\"width\"],\n","                height=coco.imgs[i][\"height\"],\n","                annotations=coco.loadAnns(coco.getAnnIds(\n","                    imgIds=[i], catIds=class_ids, iscrowd=None)))\n","        if return_coco:\n","            return coco\n","\n","    def load_keypoint(self, image_id):\n","        \"\"\"\n","        \"\"\"\n","        image_info = self.image_info[image_id]\n","        if image_info[\"source\"] != \"deepfashion2\":\n","            return super(DeepFashion2Dataset, self).load_mask(image_id)\n","\n","        instance_keypoints = []\n","        class_ids = []\n","        annotations = self.image_info[image_id][\"annotations\"]\n","\n","        for annotation in annotations:\n","            class_id = self.map_source_class_id(\n","                \"deepfashion2.{}\".format(annotation['category_id']))\n","            if class_id:\n","                keypoint = annotation['keypoints']\n","\n","                instance_keypoints.append(keypoint)\n","                class_ids.append(class_id)\n","\n","        keypoints = np.stack(instance_keypoints, axis=1)\n","        class_ids = np.array(class_ids, dtype=np.int32)\n","        return keypoints, class_ids\n","            \n","    def load_mask(self, image_id):\n","        \"\"\"Load instance masks for the given image.\n","        Different datasets use different ways to store masks. This\n","        function converts the different mask format to one format\n","        in the form of a bitmap [height, width, instances].\n","        Returns:\n","        masks: A bool array of shape [height, width, instance count] with\n","            one mask per instance.\n","        class_ids: a 1D array of class IDs of the instance masks.\n","        \"\"\"\n","        # If not a COCO image, delegate to parent class.\n","        image_info = self.image_info[image_id]\n","        if image_info[\"source\"] != \"deepfashion2\":\n","            return super(DeepFashion2Dataset, self).load_mask(image_id)\n","\n","        instance_masks = []\n","        class_ids = []\n","        annotations = self.image_info[image_id][\"annotations\"]\n","        # Build mask of shape [height, width, instance_count] and list\n","        # of class IDs that correspond to each channel of the mask.\n","        for annotation in annotations:\n","            class_id = self.map_source_class_id(\n","                \"deepfashion2.{}\".format(annotation['category_id']))\n","            if class_id:\n","                m = self.annToMask(annotation, image_info[\"height\"],\n","                                   image_info[\"width\"])\n","                # Some objects are so small that they're less than 1 pixel area\n","                # and end up rounded out. Skip those objects.\n","                if m.max() < 1:\n","                    continue\n","                # Is it a crowd? If so, use a negative class ID.\n","                if annotation['iscrowd']:\n","                    # Use negative class ID for crowds\n","                    class_id *= -1\n","                    # For crowd masks, annToMask() sometimes returns a mask\n","                    # smaller than the given dimensions. If so, resize it.\n","                    if m.shape[0] != image_info[\"height\"] or m.shape[1] != image_info[\"width\"]:\n","                        m = np.ones([image_info[\"height\"], image_info[\"width\"]], dtype=bool)\n","                instance_masks.append(m)\n","                class_ids.append(class_id)\n","\n","        # Pack instance masks into an array\n","        if class_ids:\n","            mask = np.stack(instance_masks, axis=2).astype(np.bool)\n","            class_ids = np.array(class_ids, dtype=np.int32)\n","            return mask, class_ids\n","        else:\n","            # Call super class to return an empty mask\n","            return super(DeepFashion2Dataset, self).load_mask(image_id)\n","\n","    def image_reference(self, image_id):\n","        \"\"\"Return a link to the image in the COCO Website.\"\"\"\n","        super(DeepFashion2Dataset, self).image_reference(image_id)\n","\n","    # The following two functions are from pycocotools with a few changes.\n","\n","    def annToRLE(self, ann, height, width):\n","        \"\"\"\n","        Convert annotation which can be polygons, uncompressed RLE to RLE.\n","        :return: binary mask (numpy 2D array)\n","        \"\"\"\n","        segm = ann['segmentation']\n","        if isinstance(segm, list):\n","            # polygon -- a single object might consist of multiple parts\n","            # we merge all parts into one mask rle code\n","            rles = maskUtils.frPyObjects(segm, height, width)\n","            rle = maskUtils.merge(rles)\n","        elif isinstance(segm['counts'], list):\n","            # uncompressed RLE\n","            rle = maskUtils.frPyObjects(segm, height, width)\n","        else:\n","            # rle\n","            rle = ann['segmentation']\n","        return rle\n","\n","    def annToMask(self, ann, height, width):\n","        \"\"\"\n","        Convert annotation which can be polygons, uncompressed RLE, or RLE to binary mask.\n","        :return: binary mask (numpy 2D array)\n","        \"\"\"\n","        rle = self.annToRLE(ann, height, width)\n","        m = maskUtils.decode(rle)\n","        return m\n","\n","\n","def train(model, config):\n","    \"\"\"\n","    \"\"\"\n","    dataset_train = DeepFashion2Dataset()\n","    dataset_train.load_coco(config.train_img_dir, config.train_json_path)\n","    dataset_train.prepare()\n","\n","    dataset_valid = DeepFashion2Dataset()\n","    dataset_valid.load_coco(config.valid_img_dir, config.valid_json_path)\n","    dataset_valid.prepare()\n","\n","    model.train(dataset_train, dataset_valid,\n","                learning_rate=config.LEARNING_RATE,\n","                epochs=1,\n","                layers='heads')\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AmJkMK-5q-Ou","executionInfo":{"elapsed":7024,"status":"ok","timestamp":1609860923273,"user":{"displayName":"kejkaew thanasuan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh9QTKDSZ9uv61V23eh27Gu_jztkcap_wnhDLCbcQ=s64","userId":"01212051864468347195"},"user_tz":-420},"outputId":"b207f9f7-52f4-4bb9-e879-67c285cc9a89"},"source":["config = DeepFashion2Config()\n","model = MaskRCNN(mode=\"training\", config=config,\n","                                  model_dir=MODEL_DIR)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n","Instructions for updating:\n","If using Keras pass *_constraint arguments to layers.\n","WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/keras/backend/tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n","\n","WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/array_ops.py:1475: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n","WARNING:tensorflow:From /content/Deepfashion2_Training/lib/model.py:554: The name tf.random_shuffle is deprecated. Please use tf.random.shuffle instead.\n","\n","WARNING:tensorflow:From /content/Deepfashion2_Training/lib/utils.py:202: The name tf.log is deprecated. Please use tf.math.log instead.\n","\n","WARNING:tensorflow:From /content/Deepfashion2_Training/lib/model.py:601: calling crop_and_resize_v1 (from tensorflow.python.ops.image_ops_impl) with box_ind is deprecated and will be removed in a future version.\n","Instructions for updating:\n","box_ind is deprecated, use box_indices instead\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"AfHYd8NNrlGv"},"source":["model.load_weights(COCO_MODEL_PATH, by_name=True, exclude=[\"mrcnn_class_logits\", \"mrcnn_bbox_fc\",\n","                                                        \"mrcnn_bbox\", \"mrcnn_mask\"])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VZ3WEics5Q2J","outputId":"249335b0-6744-485c-c21c-166adefb78a3"},"source":["train(model, config)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["loading annotations into memory...\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":181},"id":"g3opRcwxG3SZ","executionInfo":{"elapsed":1000,"status":"error","timestamp":1609859950117,"user":{"displayName":"kejkaew thanasuan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh9QTKDSZ9uv61V23eh27Gu_jztkcap_wnhDLCbcQ=s64","userId":"01212051864468347195"},"user_tz":-420},"outputId":"b97eb37d-ead9-4407-ac21-7b030799517d"},"source":["model_path = os.path.join(MODEL_DIR, \"mask_rcnn_deepfashion1.h5\")\n","model.keras_model.save_weights(model_path)"],"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-7441d83c9324>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMODEL_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"mask_rcnn_deepfashion1.h5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-_k3zAddP5Lv","executionInfo":{"elapsed":25601,"status":"ok","timestamp":1609862210417,"user":{"displayName":"kejkaew thanasuan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh9QTKDSZ9uv61V23eh27Gu_jztkcap_wnhDLCbcQ=s64","userId":"01212051864468347195"},"user_tz":-420},"outputId":"4074c404-d75b-4ab1-c088-dee2b641a1ba"},"source":["!wc -l ./Dataset/train/train.json"],"execution_count":null,"outputs":[{"output_type":"stream","text":["0 ./Dataset/train/train.json\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"background_save":true},"id":"urepdB9aQIx5"},"source":["!head -1 ./Dataset/train/train.json"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ak9xqd7GRgPp"},"source":[""],"execution_count":null,"outputs":[]}]}